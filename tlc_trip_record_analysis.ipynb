{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "duMRT5nX0USK"
   },
   "source": [
    "#Assignment: Spark SQL and Data Frames\n",
    "\n",
    "Dataset: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page (February 2021)\n",
    "\n",
    "Tech Stack:\n",
    "\n",
    "1.   PySpark\n",
    "2.   Google BigQuery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4OMbN6oLYDl",
    "outputId": "8928f142-3a58-40ff-afce-30e5e8268e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.0.3\n",
      "  Downloading pyspark-3.0.3.tar.gz (209.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 209.1 MB 55 kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 153.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.3-py2.py3-none-any.whl size=209435970 sha256=43bb864c589a2f4c81a3d9a6f7436e53e0d4201f1f0cd9b8e72e89be57bdd8d0\n",
      "  Stored in directory: /home/Archie/.cache/pip/wheels/65/ff/a2/0e26ceacea69c69610bbbd569678580b54be2e6e8b88e0eb9a\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.5\n",
      "    Uninstalling py4j-0.10.9.5:\n",
      "      Successfully uninstalled py4j-0.10.9.5\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.3.2\n",
      "    Uninstalling pyspark-3.3.2:\n",
      "      Successfully uninstalled pyspark-3.3.2\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9CPTXVwFKJx2"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6St1UJUcKoiA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOG6BIxS3_zs",
    "outputId": "47683b0f-a053-490b-9edc-13b739318c68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /home/Archie/anaconda3/lib/python3.9/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aJeywlyoKrbo",
    "outputId": "1125939f-dcdb-4a4d-c0f7-be9cb43b3b86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nAGZfrjcM3IC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/16 00:42:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"archiecm\").config('spark.ui.port', '4050').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "IIVHzS1_K2cH",
    "outputId": "350ba362-32f7-4348-cae2-2ab64f17faa6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://df9-instance.us-central1-a.c.data-fellowship-9-project.internal:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>archiecm</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd24c3e8e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AXe5v0tJjR1",
    "outputId": "d09a923b-a133-4073-976c-847f3c30d6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"yellow_tripdata_2021-02.parquet\", header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPvrn-DJ8Bcr",
    "outputId": "fd16b54e-3ad6-44b3-f2e3-9900e39e3ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Data: (1371709, 19)\n",
      "Rows of Data: 1371709\n",
      "Columns of Data: 19\n"
     ]
    }
   ],
   "source": [
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "\n",
    "print(f'Dimensions of Data: {(rows,cols)}')\n",
    "print(f'Rows of Data: {rows}')\n",
    "print(f'Columns of Data: {cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AIkQIYX4gRh",
    "outputId": "aa89f52b-359d-4e21-d8f9-50b2227b984f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2021-02-01 00:40:47|  2021-02-01 00:48:28|            1.0|          2.3|       1.0|                 N|         141|         226|           2|        8.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.3|                 2.5|       null|\n",
      "|       1| 2021-02-01 00:07:44|  2021-02-01 00:20:31|            1.0|          1.6|       1.0|                 N|          43|         263|           2|        9.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 0.0|       null|\n",
      "|       1| 2021-02-01 00:59:36|  2021-02-01 01:24:13|            1.0|          5.3|       1.0|                 N|         114|         263|           2|       19.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        22.8|                 2.5|       null|\n",
      "|       2| 2021-02-01 00:03:26|  2021-02-01 00:16:32|            1.0|         2.79|       1.0|                 N|         236|         229|           1|       11.0|  0.5|    0.5|      2.96|         0.0|                  0.3|       17.76|                 2.5|       null|\n",
      "|       2| 2021-02-01 00:20:20|  2021-02-01 00:24:03|            2.0|         0.64|       1.0|                 N|         229|         140|           1|        4.5|  0.5|    0.5|      1.66|         0.0|                  0.3|        9.96|                 2.5|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63sBIw570wa0"
   },
   "source": [
    "#### How many Taxi Trips were there on February 15?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WyZYyuNE65RT"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pr--L9VfFKUR"
   },
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "df.registerTempTable('data_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2hNlWvZ7WR1",
    "outputId": "549f8468-7272-47d2-bfd0-5b492030b1a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|total_taxi_trip_0215|\n",
      "+--------------------+\n",
      "|               40322|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_taxi_trip_0215 = spark.sql(\"\"\" \n",
    "\n",
    "    SELECT COUNT(pickup_datetime) AS total_taxi_trip_0215\n",
    "    FROM data_table\n",
    "    WHERE pickup_datetime >= '2021-02-15 00:00:00' AND pickup_datetime < '2021-02-16 00:00:00'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "total_taxi_trip_0215.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCvmXWe604GH"
   },
   "source": [
    "#### Find the longest trip for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0YiAx7VCKbtL"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('data_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugikekqT0NjX",
    "outputId": "d4dbd045-9af6-419b-e6f4-f6237fabdb7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 7:===============================>                       (116 + 1) / 200]\r\n",
      "\r\n",
      "[Stage 7:===============================================>       (171 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|pickup_datetime|longest_trip|\n",
      "+---------------+------------+\n",
      "|     2021-02-16|   221188.25|\n",
      "|     2021-02-20|   188054.03|\n",
      "|     2021-02-08|   186617.92|\n",
      "|     2021-02-07|   186510.67|\n",
      "|     2021-02-03|   186079.73|\n",
      "|     2021-02-17|   140145.44|\n",
      "|     2021-02-13|   115928.92|\n",
      "|     2021-02-05|    91134.16|\n",
      "|     2021-02-26|    90796.21|\n",
      "|     2021-02-24|    90073.44|\n",
      "+---------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "longesttrip_eachday = df.withColumn(\"pickup_datetime\" , to_date(df['pickup_datetime']))\\\n",
    "                      .select(['pickup_datetime','trip_distance'])\\\n",
    "                      .where(\"pickup_datetime >= '2021-02-01' \")\\\n",
    "                      .groupby(F.col('pickup_datetime')).agg(F.max('trip_distance').alias('longest_trip')).sort(desc(\"longest_trip\"))\n",
    "longesttrip_eachday.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPZyFzW71Aqj"
   },
   "source": [
    "#### Find top 5 most frequent 'dispatching_base_num'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzvxfl9ZNgZb",
    "outputId": "81379ae2-8aee-4183-b5ad-3112226376be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: double (nullable = true)\n",
      " |-- DOlocationID: double (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv = spark.read.parquet(\"fhv_tripdata_2021-02.parquet\", header=True, inferSchema=True)\n",
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PhcKxOhPAq5",
    "outputId": "119df5df-eeeb-4641-c5a5-ef86f5e20742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Data: (1037692, 7)\n",
      "Rows of Data: 1037692\n",
      "Columns of Data: 7\n"
     ]
    }
   ],
   "source": [
    "rows = df_fhv.count()\n",
    "cols = len(df_fhv.columns)\n",
    "\n",
    "print(f'Dimensions of Data: {(rows,cols)}')\n",
    "print(f'Rows of Data: {rows}')\n",
    "print(f'Columns of Data: {cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cig_M-oXPHAd",
    "outputId": "016aa501-f871-49f5-9cc2-a27dcf8304c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00013|2021-02-01 00:01:00|2021-02-01 01:33:00|        null|        null|   null|                B00014|\n",
      "|     B00021         |2021-02-01 00:55:40|2021-02-01 01:06:20|       173.0|        82.0|   null|       B00021         |\n",
      "|     B00021         |2021-02-01 00:14:03|2021-02-01 00:28:37|       173.0|        56.0|   null|       B00021         |\n",
      "|     B00021         |2021-02-01 00:27:48|2021-02-01 00:35:45|        82.0|       129.0|   null|       B00021         |\n",
      "|              B00037|2021-02-01 00:12:50|2021-02-01 00:26:38|        null|       225.0|   null|                B00037|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NXwl6Fg1F7L",
    "outputId": "97f98047-7f0a-49c8-814f-4a993beda19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|dispatching_base_num|count|\n",
      "+--------------------+-----+\n",
      "|              B00856|35077|\n",
      "|              B01312|33089|\n",
      "|              B01145|31114|\n",
      "|              B02794|30397|\n",
      "|              B03016|29794|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 13:==========================================>           (159 + 1) / 200]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top5_frequent_dbm = df_fhv.groupBy(\"dispatching_base_num\").count() \\\n",
    "                    .orderBy(F.col('count').desc())\n",
    "\n",
    "top5_frequent_dbm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTgeGRyV1OEQ"
   },
   "source": [
    "#### Find top 5 most common location pairs (PULocationID and DOLocationID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtDnw0aQQyHV",
    "outputId": "6cd953e4-597e-465b-b401-08a99e6ee934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+\n",
      "|PUlocationID|DOlocationID|count|\n",
      "+------------+------------+-----+\n",
      "|         237|         236|11455|\n",
      "|         236|         237| 9901|\n",
      "|         236|         236| 8819|\n",
      "|         237|         237| 7324|\n",
      "|         264|         264| 5732|\n",
      "+------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 15:=================================================>    (183 + 1) / 200]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top5_location_pairs = df.where(\"PUlocationID IS NOT NULL AND DOlocationID IS NOT NULL\") \\\n",
    "                      .groupBy([\"PUlocationID\",'DOlocationID']) \\\n",
    "                      .count() \\\n",
    "                      .orderBy(F.col('count').desc())\n",
    "top5_location_pairs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SspRtFwg1VZ0",
    "outputId": "c8166004-fb85-49d3-d18c-102d060847bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+\n",
      "|PUlocationID|DOlocationID|count|\n",
      "+------------+------------+-----+\n",
      "|       206.0|       206.0| 2374|\n",
      "|       221.0|       206.0| 2112|\n",
      "|       129.0|       129.0| 1902|\n",
      "|         7.0|         7.0| 1829|\n",
      "|       179.0|       179.0| 1736|\n",
      "+------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_location_pairs_fhv = df_fhv.where(\"PUlocationID IS NOT NULL AND DOlocationID IS NOT NULL\") \\\n",
    "                          .groupBy([\"PUlocationID\",'DOlocationID']) \\\n",
    "                          .count() \\\n",
    "                          .orderBy(F.col('count').desc())\n",
    "top5_location_pairs_fhv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQBdb59i1WJ1"
   },
   "source": [
    "#### Write all of the result to BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \".google/credentials/google_credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_bucket = 'fellowship_9_staging'\n",
    "bq_dataset = 'taxi_trip_spark'\n",
    "bq_table = 'taxi_trip_02_2021'\n",
    "\n",
    "df.write.format(\"bigquery\") \\\n",
    "  .option(\"table\",\"{}.{}\".format(bq_dataset, bq_table)) \\\n",
    "  .option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    "  .mode('overwrite') \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "63sBIw570wa0",
    "wCvmXWe604GH",
    "jPZyFzW71Aqj",
    "MTgeGRyV1OEQ",
    "KQBdb59i1WJ1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
